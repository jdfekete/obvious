Obvious is a set of interfaces and extension classes for wrapping
around existing InfoVis toolkits.  It generalizes and extends the
standard architecture as defined in the InfoVis reference model to try
to abstract all the existing implementations.  In this section, we
list some major existing toolkits and explain what they share and how
they differ.  In the second section, we describe the most common
standardization processes for software systems.

\subsection{Visualization Toolkits}

Pretty much all existing InfoVis toolkits follow the
InfoVis reference model initially specified by Ed Chi and refined by
Card, Mackinlay and Shneiderman~\cite{ReadingsIV,ChiRefModel} and has
been described as a design pattern in~\cite{DesignPatternsIV}.  The
model defines three stages: \emph{DataSet} or \emph{Data Tables},
\emph{Visualization} or \emph{Visual Structure}, and \emph{View}
(Figure~\ref{fig:refmodel}).  One of its main benefits is that it
explicitly represents interaction, in contrast to older visualization:
models.  Several articles have described the concrete design of an
InfoVis toolkit.  We report here on the common and
the specific parts.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/refmodpat}
\caption{The Information Visualization Reference Model~\cite{DesignPatternsIV}}
\label{fig:refmodel}
\end{figure}


The InfoVis Toolkit (IVTK)~\cite{InfoVis} implements an
\emph{in-memory database manager} where data is organized in columns,
contrary to most persistent relational databases, to improve the
memory footprint, to allow addition of new attributes that are needed
to manage the interaction (e.g., selection or filtering), and to hold
attributes computed on demand; the main challenge being the support of
interactive performance for rendering and dynamic queries with a small
memory footprint.  The visual structure is managed using a
\emph{monolithic} architecture~\cite{Polylithic}: each visualization
technique is implemented as a specific class (e.g.,
ScatterPlotVisualization, ParallelCoordinatesVisualization, or
TreeVisualization) that performs the mapping between the data set and
the graphic items to render.  Finally, the view component is the same
for each of the visual structures and takes care of scrolling,
zooming, and overlaying magic lenses (e.g. Fisheye or Magic Lenses).
A \emph{notification mechanism} implements the communication between
the data tables and the visual structures: each time a data table is
modified, it notifies all the registered handlers of the details of
the modification. The interaction is managed by \emph{Interactor}
objects that are associated with the visual structures; the views are
generic and forward interaction managements to the Interactors.  One
specific feature provided by IVTK is layering: visualizations can be
stacked on top of each others.  Composite visualizations are useful
to build complex visualizations by breaking them into simple
parts. For example, node-link diagrams are split into links managed as
one layer and nodes as another.  Magic lenses and Fisheyes are also
managed as layers on top of other visualizations.

Prefuse~\cite{Prefuse} also relies on an in-memory database with
notifications but implements the visual structure using an extension of
the data model (a visual table is derived from a data table).  It then
transforms the data into a \emph{polylithic} graphic structure whereas
all the other toolkits use a \emph{monolithic} architecture.  In a
polylithic architecture, there is only one component in charge of all
the visual structures.  A visualization object is responsible of
managing a visual structure: it contains visual tables that augment
data tables with graphic attributes (shape, color, etc.).
Visualizations are in charge of computing the layout (assigning a
position and shape to visual items), the graphic attributes, and
animations.  Visualizations use a \emph{Renderer} object to actually
display visual items.  Users can control which renderer is used
depending on the visualization, and the object itself.  In Prefuse,
data managers, visual managers and views are generic, offering a
clean interface to the application programmer.  However, as noted by
Bederson et al.~\cite{Polylithic}, polylithic toolkits have a steeper
learning curve than monolithic ones because the polylithic components
do not work out of the box, they always need to be configured.  To
address this issue, Prefuse comes with code samples that show how to
do the initial setup.

Building upon their experience in the Prefuse toolkit~\cite{Prefuse},
Heer and Agrawala~\cite{DesignPatternsIV} have derived software design
patterns that are common to InfoVis applications, and toolkits. 

Improvise~\cite{Improvise} relies on an in-memory database with
notification that is row-oriented and its visual structures are
monolithic.  The main characteristic of Improvise lies in its
management of coordinated views.  To achieve this aim, it relies on several
design patterns not supported by Prefuse; compared to the other
InfoVis toolkits, it adds a coordination component
that is central and extends the notification mechanism implemented
by IVTK or Prefuse.

Discovery~\cite{Discovery2,Discovery1,Discovery3} shares most of its
characteristics with Prefuse: it uses an in-memory, column-oriented
database and a polylithic graphic model. Its two main features are (1)
the absence of a scene graph, replaced by a dataflow pipeline made of
short operations called \emph{functors} that render directly from the
data-model, and (2) a deferred notification strategy to allow data
editing.

%% \jo{I'm not sure I am convinced of the assertion in the following
%%   paragraph. The three toolkits above are described in some detail,
%%   not all of which is relevant to other toolkits. Perhaps we should
%%   instead abstract their distinct design characters. e.g. Row-oriented
%%   vs column oriented; in-memory vs cached database management;
%%   monolithic vs polylithic etc. The point being that while they all
%%   attempt to achieve similar general aims, their lower-level approach
%%   to doing so requires different programming approaches, hence the
%%   need for Obvious.}

Other InfoVis toolkits can mostly be described using the four toolkits
above, even if they use a different programming language.
Tulip~\cite{Tulip} is a graph-oriented toolkit programmed in C++ that
uses data tables for vertices and edges, like IVTK and Prefuse.  It
implements several complex graph layout algorithms and uses OpenGL for
its rendering, but the conceptual architecture is table-based and
monolithic.  Therefore, InfoVis toolkits share a
global organization: they all implement an in-memory database with two
variants (row-based or column-based), a visual structure with two
variants (monolithic or polylithic), and several specific features.
Even if some choices made by toolkits designers were carefully
decided, others were probably made without being aware of alternatives.
Combining the best possible features for a next-generation toolkit
might be tempting but there are still tradeoffs that cannot be solved.
For example, the power of coordinated and linked views offered by
Improvise comes at the cost of maintaining caches that should be
flushed when the data change so more research is needed to maintain
linked and coordinated views on dynamic data.

There are also lower-level toolkits that can be used to build VA
applications.  Two popular families are graphics libraries
and graph libraries, which we discuss next.

%% \jo{I'm not sure what point is being made here with the discussion of
%%   lower level visualization toolkits. Are we asserting that the
%%   standardization implied by Obvious is also appropriate for lower
%%   level approaches to VA software construction? If so, we need to
%%   identify what is lacking in an approach without Obvious as well as
%%   demonstrating (later on in the paper) that implementing the Obvious
%%   interfaces in lower level visualization environments is both
%%   practical and beneficial. An interesting test case might be
%%   \emph{Processing} (processing.org). This is, compared to the other
%%   examples cited, a very low level approach to visualization software
%%   development. However, it is designed for rapid-prototyping, and if
%%   it could be easily integrated with Obvious it might provide a nice
%%   example of how early prototypes could be transformed into more
%%   robust applications using Obvious as the bridge. I'd be happy to
%%   write some words on this if you think it fits well with the theme of
%%   the paper.}

\subsection{Graphics Libraries}

VA applications can manage their own data structure and
take care of the mapping from data to visualization on their own.  At
this point, they can use \emph{scene-graphs} or \emph{direct-graphics}
libraries.  

Scene-Graph toolkits can manage the visual structure and view as
described in the reference model.  They are focused on computer
graphics and interaction: they only deal with the visual structure and
view.  Piccolo and Jazz~\cite{Polylithic} are popular 2D scene-graph
managers that have been used to create several information
visualization applications (e.g., ~\cite{Geneaquilt,SpaceTree}.) An
early version of Piccolo has also been used as graphics engine for the
Cytoscape graph visualization system~\cite{Cytoscape} but dropped for
performance reasons.

High-performance InfoVis applications use scene-graph optimization
techniques to speed-up the rendering of scenes.  Tulip~\cite{Tulip}
and Gephi~\cite{Gephi} maintain a spatial indexing structure to avoid
rendering objects that are not visible.

Although scene-graph technologies are mature and used in a wide
variety of graphics applications such as games, virtual-reality
applications and scientific visualization systems, they are not always
adequate for InfoVis systems because they require the explicit
specification of geometry and graphic attributes for each displayed
objects.  Very often, InfoVis can quickly compute graphic attributes
and even geometry from data attributes.  For example, the position of
an item using a scatter-plot visualization is computed using a simple
affine transformation the data attributes using the $x$ and $y$
dimensions.  There is no need to store the computed values when
computing them on the fly is cheap.  The same is true for color and
other properties.  In contrast, copying and storing this information
is costly in terms of time and memory.

Direct-graphics libraries such as \emph{Processing} or \emph{OpenGL}
can also be used to implement the visualization techniques. 
%Processing
%is well known for it rapid prototyping capabilities whereas OpenGL is
%notorious for its high-performances.

%[Add a section on Processing]

Still, when separating the data-model from the visual model,
scene-graph managers offer more flexibility than information
visualization systems for complex graphics and sophisticated
interaction.  This is why several InfoVis systems
still use them.

% say something about the fact that scene-graph toolkits are
% classically for 3D scenes.

\subsection{Graph Libraries}

While most table-based visualization toolkits rely on an in-memory
database, several graph-based visualization systems manage their
data-structures using a model inspired from graph theory where
topology is the main focus and data associated with graph entities is
less important.  This is the case for the JUNG library~\cite{jung2003}
or the Boost Graph Library (BGL)~\cite{BGL}, as well as for the graph
library used by Cytoscape~\cite{Cytoscape}.

These libraries support graphs as set of vertices and edges (the
topological entities) that can be associated with arbitrary data.
This data is just stored by the graph entities as a convenience for
the application: the library does not implement any integrity check
between data and graph entities.  In contrast, IVTK, Prefuse, and
Tulip maintain a close consistency between graphs and data tables:
removing a data table entry associated with a graph entity (vertex or
edge) also removes the entity from the graph structure.

Thus, there is no clear consensus on how a graph data structure should
be managed internally; the design choices are quite different
depending on the communities such as graph theory, information
visualization, database, and semantic web.


\subsection{Standardization Processes}

Standardization is a well established habit in the software community;
several standardization models have been used in the past and these
models tend to evolve due to the accelerating pace of software
development taking place nowadays.

According to Wikipedia: ``The goals of standardization can be to help
with independence of single suppliers (commoditization),
compatibility, interoperability, safety, repeatability, or quality.''
The goals raised in this article are well among them: compatibility,
interoperability, and quality.

Standardization roughly follows four models: 
\begin{enumerate}[noitemsep]
\item Specified by national and international organization such as the
  International Organization for Standardization (e.g. ISO, ASCII),

\item Specified by a private or public consortium (e.g. the Unicode
  Consortium, the OMG, and the World Wide Web Consortium (W3C)). Closer to
  the InfoVis community, ``The Open Geospatial
  Consortium''~\cite{OpenGeospatial} which is an international
  industry consortium of companies, government agencies, and
  universities participating in a consensus process to develop
  publicly available interface standards for geospatial data.

\item Community-driven: looser groups can be faster and allow for more
  experiments than formal standardization bodies or consortia.
  Communities, such as the \emph{Boost
    Community}~\cite{Boost} --- designing
  libraries for the ISO C++ language --- experiment, develop, and
  document software that sometimes become part of formal ISO
  standards.  The \emph{Java Community
    Process}~\cite{JCP} plays a similar role
  for the Java language and programming environment.

  Standards specified by established organizations go through a formal
  process that take substantial time; usually years.  On the other
  side, ad-hoc organizations such as consortia can issue standards or
  recommendations faster.  In particular, the W3C or community-drive
  consortia define stages for their ``recommendations'' (the name for
  standard issued by the W3C) before they are considered final.  In
  all cases, these organizations establish steering committees to
  control the processes and require substantial involvements from many
  organizations to achieve standards.

\item \textit{De facto}: At the other extreme are the application
  domains where one system becomes the standard.  For example, in
  scientific visualization, VTK~\cite{VTK} has become, in the latest
  years, the \textit{de facto} standard toolkit: it is used by
  researchers and practitioners, and newer solutions are getting
  integrated quickly into VTK.  This is possible when either a
  system reaches a certain level of popularity --- such as Microsoft
  Word for Word Processors --- or when the quality and features of the
  system are unmatched --- such as VTK.
\end{enumerate}

At the current stage, InfoVis toolkits are not understood well-enough
to start a formal standardization process.  It seems that one toolkit
will not become a \textit{de facto} standard due to the stretch in
scope and capabilities of existing toolkits.  Two models remain:
consortium-driven or community driven.  Obvious has started as a
community-driven initiative.  It is up to the community to decide how
it wants to coordinate its software development for better
compatibility, interoperability, and quality.
